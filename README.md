# PARALLELIZATION-OF-NEURAL-NETWORKS

ABSTRACT:The term neural network was used to refer to a network or circuit of neurons. The modern usage of the term often refers to artificial neural networks, which are composed of artificial neurons or nodes. It is designed to make the computer behave like a human being by giving it an ability to visualise things on its own. A neural network (NN) is used to learn the dynamics under the contraction analysis constraints. It is achieved by many models. The basic idea is to train the network with all possible inputs such that it can predict the future. An integral part of human-robot collaboration is the ability of the robot to understand and predict human motion. Predicting what the human collaborator will do next is very useful in planning the robot’s response. The prediction can occur either by telling that this belongs to the set or not i.e., classification or predicting future. Parallelization of neural networking means reducing the time the machine takes to analyse the network. It is achieved by using certain Openmp techniques which can be used to reduce time by equally distributing the work among threads. A Multilayer perceptron (MLP) is a class of feedforward artificial neural networks. There are 6 different types of parallelism that can be implemented in the training process of MLPs. In this project, we are going to implement a sequential model for training of a neural network. The sequential model is used for the training of the network by forward propagation and backward propagation respectively. Here, we are going to implement

Node Parallelism – where each node in the network is being assigned to different threads.
Data Parallelism – where the training data is equally distributed to different threads. We the analyze the time taken and hence give the estimated results.
